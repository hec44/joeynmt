{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext.data import Dataset, Iterator, Field\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_fun = lambda s: s.split()\n",
    "\n",
    "src_field = data.Field(init_token=None, eos_token=\"<EOS>\",\n",
    "                           pad_token=\"<PAD>\", tokenize=tok_fun,\n",
    "                           batch_first=True, lower=True,\n",
    "                           unk_token=\"<UNK>\",\n",
    "                           include_lengths=True)\n",
    "\n",
    "trg_field = data.Field(init_token=\"<BOS>\", eos_token=\"<EOS>\",\n",
    "                           pad_token=\"<PAD>\", tokenize=tok_fun,\n",
    "                           unk_token=\"<UNK>\",\n",
    "                           batch_first=True, lower=True,\n",
    "                           include_lengths=True)\n",
    "edge_org_field = data.Field(init_token=None, eos_token=\"<EOS>\",\n",
    "                           pad_token=\"<PAD>\", tokenize=tok_fun,\n",
    "                           unk_token=\"<UNK>\",\n",
    "                           batch_first=True, lower=True,\n",
    "                           include_lengths=True)\n",
    "edge_trg_field = data.Field(init_token=None, eos_token=\"<EOS>\",\n",
    "                           pad_token=\"<PAD>\", tokenize=tok_fun,\n",
    "                           unk_token=\"<UNK>\",\n",
    "                           batch_first=True, lower=True,\n",
    "                           include_lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.field.Field object at 0x7fb5607da6a0>\n"
     ]
    }
   ],
   "source": [
    "print(src_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TranslationDataset(data.Dataset):\n",
    "    \"\"\"Defines a dataset for machine translation.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return data.interleave_keys(len(ex.src), len(ex.trg))\n",
    "\n",
    "    def __init__(self, src_file, trg_file, fields, **kwargs):\n",
    "        \"\"\"Create a TranslationDataset given paths and fields.\n",
    "        Arguments:\n",
    "            path: Common prefix of paths to the data files for both languages.\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            Remaining keyword arguments: Passed to the constructor of\n",
    "                data.Dataset.\n",
    "        \"\"\"\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            fields = [('src', fields[0]), ('trg', fields[1]),\\\n",
    "                     ('edge_org', fields[2]), ('edge_trg', fields[3])]\n",
    "\n",
    "        examples = []\n",
    "\n",
    "        for src_line, trg_line in zip(src_file, trg_file):\n",
    "                src_line, trg_line = src_line.strip(), trg_line.strip()\n",
    "                if src_line != '' and trg_line != '':\n",
    "                    print(src_line)\n",
    "                    print(trg_line)\n",
    "                    examples.append(data.Example.fromlist(\n",
    "                        [src_line, trg_line,\"0 1 2 3 4\",\"1 2 3 4 0\"], fields))\n",
    "\n",
    "        super(TranslationDataset, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, exts, fields, path=None, root='.data',\n",
    "               train='train', validation='val', test='test', **kwargs):\n",
    "        \"\"\"Create dataset objects for splits of a TranslationDataset.\n",
    "        Arguments:\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            path (str): Common prefix of the splits' file paths, or None to use\n",
    "                the result of cls.download(root).\n",
    "            root: Root dataset storage directory. Default is '.data'.\n",
    "            train: The prefix of the train data. Default: 'train'.\n",
    "            validation: The prefix of the validation data. Default: 'val'.\n",
    "            test: The prefix of the test data. Default: 'test'.\n",
    "            Remaining keyword arguments: Passed to the splits method of\n",
    "                Dataset.\n",
    "        \"\"\"\n",
    "        if path is None:\n",
    "            path = cls.download(root)\n",
    "\n",
    "        train_data = None if train is None else cls(\n",
    "            os.path.join(path, train), exts, fields, **kwargs)\n",
    "        val_data = None if validation is None else cls(\n",
    "            os.path.join(path, validation), exts, fields, **kwargs)\n",
    "        test_data = None if test is None else cls(\n",
    "            os.path.join(path, test), exts, fields, **kwargs)\n",
    "        return tuple(d for d in (train_data, val_data, test_data)\n",
    "                     if d is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_UNK_ID = lambda: 0\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "BOS_TOKEN = \"<BOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "class Vocabulary:\n",
    "    \"\"\" Vocabulary represents mapping between tokens and indices. \"\"\"\n",
    "\n",
    "    def __init__(self, tokens = None, file: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Create vocabulary from list of tokens or file.\n",
    "\n",
    "        Special tokens are added if not already in file or list.\n",
    "        File format: token with index i is in line i.\n",
    "\n",
    "        :param tokens: list of tokens\n",
    "        :param file: file to load vocabulary from\n",
    "        \"\"\"\n",
    "        # don't rename stoi and itos since needed for torchtext\n",
    "        # warning: stoi grows with unknown tokens, don't use for saving or size\n",
    "\n",
    "        # special symbols\n",
    "        self.specials = [\"<UNK>\", \"<PAD>\", \"<BOS>\", \"<EOS>\"]\n",
    "\n",
    "        self.stoi = defaultdict(DEFAULT_UNK_ID)\n",
    "        self.itos = []\n",
    "        if tokens is not None:\n",
    "            self._from_list(tokens)\n",
    "        elif file is not None:\n",
    "            self._from_file(file)\n",
    "\n",
    "    def _from_list(self, tokens = None) -> None:\n",
    "        \"\"\"\n",
    "        Make vocabulary from list of tokens.\n",
    "        Tokens are assumed to be unique and pre-selected.\n",
    "        Special symbols are added if not in list.\n",
    "\n",
    "        :param tokens: list of tokens\n",
    "        \"\"\"\n",
    "        self.add_tokens(tokens=self.specials+tokens)\n",
    "        assert len(self.stoi) == len(self.itos)\n",
    "\n",
    "    def _from_file(self, file: str) -> None:\n",
    "        \"\"\"\n",
    "        Make vocabulary from contents of file.\n",
    "        File format: token with index i is in line i.\n",
    "\n",
    "        :param file: path to file where the vocabulary is loaded from\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        with open(file, \"r\") as open_file:\n",
    "            for line in open_file:\n",
    "                tokens.append(line.strip(\"\\n\"))\n",
    "        self._from_list(tokens)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.stoi.__str__()\n",
    "\n",
    "    def to_file(self, file: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the vocabulary to a file, by writing token with index i in line i.\n",
    "\n",
    "        :param file: path to file where the vocabulary is written\n",
    "        \"\"\"\n",
    "        with open(file, \"w\") as open_file:\n",
    "            for t in self.itos:\n",
    "                open_file.write(\"{}\\n\".format(t))\n",
    "\n",
    "    def add_tokens(self, tokens) -> None:\n",
    "        \"\"\"\n",
    "        Add list of tokens to vocabulary\n",
    "\n",
    "        :param tokens: list of tokens to add to the vocabulary\n",
    "        \"\"\"\n",
    "        for t in tokens:\n",
    "            new_index = len(self.itos)\n",
    "            # add to vocab if not already there\n",
    "            if t not in self.itos:\n",
    "                self.itos.append(t)\n",
    "                self.stoi[t] = new_index\n",
    "\n",
    "    def is_unk(self, token: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether a token is covered by the vocabulary\n",
    "\n",
    "        :param token:\n",
    "        :return: True if covered, False otherwise\n",
    "        \"\"\"\n",
    "        return self.stoi[token] == DEFAULT_UNK_ID()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.itos)\n",
    "\n",
    "    def array_to_sentence(self, array: np.array, cut_at_eos=True,\n",
    "                          skip_pad=True):\n",
    "        \"\"\"\n",
    "        Converts an array of IDs to a sentence, optionally cutting the result\n",
    "        off at the end-of-sequence token.\n",
    "\n",
    "        :param array: 1D array containing indices\n",
    "        :param cut_at_eos: cut the decoded sentences at the first <eos>\n",
    "        :param skip_pad: skip generated <pad> tokens\n",
    "        :return: list of strings (tokens)\n",
    "        \"\"\"\n",
    "        sentence = []\n",
    "        for i in array:\n",
    "            s = self.itos[i]\n",
    "            if cut_at_eos and s == EOS_TOKEN:\n",
    "                break\n",
    "            if skip_pad and s == PAD_TOKEN:\n",
    "                continue\n",
    "            sentence.append(s)\n",
    "        return sentence\n",
    "\n",
    "    def arrays_to_sentences(self, arrays: np.array, cut_at_eos=True,\n",
    "                            skip_pad=True):\n",
    "        \"\"\"\n",
    "        Convert multiple arrays containing sequences of token IDs to their\n",
    "        sentences, optionally cutting them off at the end-of-sequence token.\n",
    "\n",
    "        :param arrays: 2D array containing indices\n",
    "        :param cut_at_eos: cut the decoded sentences at the first <eos>\n",
    "        :param skip_pad: skip generated <pad> tokens\n",
    "        :return: list of list of strings (tokens)\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        for array in arrays:\n",
    "            sentences.append(\n",
    "                self.array_to_sentence(array=array, cut_at_eos=cut_at_eos,\n",
    "                                       skip_pad=skip_pad))\n",
    "        return sentences\n",
    "\n",
    "\n",
    "def build_vocab(field: str, max_size: int, min_freq: int, dataset: Dataset,\n",
    "                vocab_file: str = None) -> Vocabulary:\n",
    "    \"\"\"\n",
    "    Builds vocabulary for a torchtext `field` from given`dataset` or\n",
    "    `vocab_file`.\n",
    "\n",
    "    :param field: attribute e.g. \"src\"\n",
    "    :param max_size: maximum size of vocabulary\n",
    "    :param min_freq: minimum frequency for an item to be included\n",
    "    :param dataset: dataset to load data for field from\n",
    "    :param vocab_file: file to store the vocabulary,\n",
    "        if not None, load vocabulary from here\n",
    "    :return: Vocabulary created from either `dataset` or `vocab_file`\n",
    "    \"\"\"\n",
    "\n",
    "    if vocab_file is not None:\n",
    "        # load it from file\n",
    "        vocab = Vocabulary(file=vocab_file)\n",
    "    else:\n",
    "        # create newly\n",
    "        def filter_min(counter: Counter, min_freq: int):\n",
    "            \"\"\" Filter counter by min frequency \"\"\"\n",
    "            filtered_counter = Counter({t: c for t, c in counter.items()\n",
    "                                        if c >= min_freq})\n",
    "            return filtered_counter\n",
    "\n",
    "        def sort_and_cut(counter: Counter, limit: int):\n",
    "            \"\"\" Cut counter to most frequent,\n",
    "            sorted numerically and alphabetically\"\"\"\n",
    "            # sort by frequency, then alphabetically\n",
    "            tokens_and_frequencies = sorted(counter.items(),\n",
    "                                            key=lambda tup: tup[0])\n",
    "            tokens_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "            vocab_tokens = [i[0] for i in tokens_and_frequencies[:limit]]\n",
    "            return vocab_tokens\n",
    "\n",
    "        tokens = []\n",
    "        for i in dataset.examples:\n",
    "            if field == \"src\":\n",
    "                tokens.extend(i.src)\n",
    "            elif field == \"trg\":\n",
    "                tokens.extend(i.trg)\n",
    "\n",
    "        counter = Counter(tokens)\n",
    "        if min_freq > -1:\n",
    "            counter = filter_min(counter, min_freq)\n",
    "        vocab_tokens = sort_and_cut(counter, max_size)\n",
    "        assert len(vocab_tokens) <= max_size\n",
    "\n",
    "        vocab = Vocabulary(tokens=vocab_tokens)\n",
    "        assert len(vocab) <= max_size + len(vocab.specials)\n",
    "        assert vocab.itos[DEFAULT_UNK_ID()] == UNK_TOKEN\n",
    "\n",
    "    # check for all except for UNK token whether they are OOVs\n",
    "    for s in vocab.specials[1:]:\n",
    "        assert not vocab.is_unk(s)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy to be here\n",
      "estoy feliz de estar aquí\n",
      "This is the test sentence number 2\n",
      "ésta es la oración de prueba número 2\n",
      "the last test sentence is  this one\n",
      "La última ouración de prueba es ésta\n"
     ]
    }
   ],
   "source": [
    "english_sentences=[\"I am happy to be here\",\"This is the test sentence number 2\",\"the last test sentence is  this one\"]\n",
    "spanish_sentences=[\"estoy feliz de estar aquí\",\"ésta es la oración de prueba número 2\",\"La última ouración de prueba es ésta\"]\n",
    "edge_indexes=[[[0,1,2,3,4,5],[0,1,2,3,4,5]],[[0,1,2,3,4],[0,1,2,3,4]],[[0,1,2,3],[0,1,2,3]]]\n",
    "dataset=TranslationDataset(english_sentences,spanish_sentences,fields=(src_field, trg_field,edge_org_field,edge_trg_field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = build_vocab(field=\"src\", min_freq=1,\n",
    "                            max_size=99,\n",
    "                            dataset=dataset, vocab_file=None)\n",
    "trg_vocab = build_vocab(field=\"trg\", min_freq=1,\n",
    "                            max_size=99,\n",
    "                            dataset=dataset, vocab_file=None)\n",
    "edge_org_vocab = build_vocab(field=\"edge_org\", min_freq=1,\n",
    "                            max_size=99,\n",
    "                            dataset=dataset, vocab_file=None)\n",
    "edge_trg_vocab = build_vocab(field=\"edge_trg\", min_freq=1,\n",
    "                            max_size=99,\n",
    "                            dataset=dataset, vocab_file=None)\n",
    "src_field.vocab = src_vocab\n",
    "trg_field.vocab = trg_vocab\n",
    "edge_org_field.vocab = edge_org_vocab\n",
    "edge_trg_field.vocab = edge_trg_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'<UNK>': 0,\n",
       "             '<PAD>': 1,\n",
       "             '<BOS>': 2,\n",
       "             '<EOS>': 3,\n",
       "             '1': 0,\n",
       "             '2': 0,\n",
       "             '3': 0,\n",
       "             '4': 0,\n",
       "             '0': 0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_trg_vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = data.BucketIterator(\n",
    "            repeat=False, sort=False, dataset=dataset,\n",
    "            batch_size=3, batch_size_fn=None,\n",
    "            train=True, sort_within_batch=True,\n",
    "            sort_key=lambda x: len(x.src), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0, 0, 0, 3],\n",
       "         [0, 0, 0, 0, 0, 3],\n",
       "         [0, 0, 0, 0, 0, 3]]),\n",
       " tensor([6, 6, 6]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.edge_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fields = [('edge_org', data.Field(sequential=False)), ('edge_trg', data.Field(sequential=False))]\n",
    "data.Example.fromlist([[0,1,2,3,4,5],\\\n",
    "                       [1,2,3,4,5,0]],\\\n",
    "                      fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex=data.Field(sequential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
