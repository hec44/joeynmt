{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext.data import Dataset, Iterator, Field\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hec44/.local/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "tok_fun = lambda s: s.split()\n",
    "\n",
    "src_field = data.Field(init_token=None, eos_token=\"<EOS>\",\n",
    "                           pad_token=\"<PAD>\", tokenize=tok_fun,\n",
    "                           batch_first=True, lower=True,\n",
    "                           unk_token=\"<UNK>\",\n",
    "                           include_lengths=True)\n",
    "\n",
    "trg_field = data.Field(init_token=\"<BOS>\", eos_token=\"<EOS>\",\n",
    "                           pad_token=\"<PAD>\", tokenize=tok_fun,\n",
    "                           unk_token=\"<UNK>\",\n",
    "                           batch_first=True, lower=True,\n",
    "                           include_lengths=True)\n",
    "edge_org_field = data.Field(init_token=None, eos_token=None,\n",
    "                           pad_token=None ,tokenize=tok_fun,\n",
    "                           unk_token=None,\n",
    "                           batch_first=True, lower=True,\n",
    "                           include_lengths=True)\n",
    "edge_trg_field = data.Field(init_token=None, eos_token=None,\n",
    "                           pad_token=None, tokenize=tok_fun,\n",
    "                           unk_token=None,\n",
    "                           batch_first=True, lower=True,\n",
    "                           include_lengths=True)\n",
    "positional_en_field = data.Field(init_token=None, eos_token=None,\n",
    "                           pad_token=None, tokenize=tok_fun,\n",
    "                           unk_token=None,\n",
    "                           batch_first=True, lower=True,\n",
    "                           include_lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.field.Field object at 0x7f178eacdf10>\n"
     ]
    }
   ],
   "source": [
    "print(src_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTranslationDataset(data.Dataset):\n",
    "    \"\"\"Defines a dataset for machine translation with a graph reprsentation on the input and levi graph transformations.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return data.interleave_keys(len(ex.src), len(ex.trg))\n",
    "\n",
    "    def __init__(self, src_file, trg_file, fields, **kwargs):\n",
    "        \"\"\"Create a TranslationDataset given paths and fields.\n",
    "        Arguments:\n",
    "            path: Common prefix of paths to the data files for both languages.\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            Remaining keyword arguments: Passed to the constructor of\n",
    "                data.Dataset.\n",
    "        \"\"\"\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            fields = [('src', fields[0]), ('trg', fields[1]),\\\n",
    "                     ('edge_org', fields[2]), ('edge_trg', fields[3]), ('positional_en',fields[4])]\n",
    "\n",
    "        examples = []\n",
    "        source_words,origins,targets=self.read_conllu(src_file)\n",
    "        pes=self.gen_pes(source_words,origins,targets)\n",
    "        \n",
    "        target_words=self.read_text_file(trg_file)\n",
    "        assert len(source_words)==len(target_words),\"Mismatch of source and tagret sentences\"\n",
    "        print(pes)\n",
    "        print(targets)\n",
    "        print(origins)\n",
    "        for i in range(len(source_words)):\n",
    "                src_line, trg_line = \" \".join(source_words[i]),target_words[i]\n",
    "                src_line, trg_line = src_line.strip(), trg_line.strip()\n",
    "                \n",
    "                if src_line != '' and trg_line != '':\n",
    "                    examples.append(data.Example.fromlist(\n",
    "                        [src_line, trg_line,\" \".join(origins[i]),\" \".join(targets[i]),\\\n",
    "                        \" \".join(pes[i])],\\\n",
    "                        fields))\n",
    "        super(GraphTranslationDataset, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, exts, fields, path=None, root='.data',\n",
    "               train='train', validation='val', test='test', **kwargs):\n",
    "        \"\"\"Create dataset objects for splits of a TranslationDataset.\n",
    "        Arguments:\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            path (str): Common prefix of the splits' file paths, or None to use\n",
    "                the result of cls.download(root).\n",
    "            root: Root dataset storage directory. Default is '.data'.\n",
    "            train: The prefix of the train data. Default: 'train'.\n",
    "            validation: The prefix of the validation data. Default: 'val'.\n",
    "            test: The prefix of the test data. Default: 'test'.\n",
    "            Remaining keyword arguments: Passed to the splits method of\n",
    "                Dataset.\n",
    "        \"\"\"\n",
    "        if path is None:\n",
    "            path = cls.download(root)\n",
    "\n",
    "        train_data = None if train is None else cls(\n",
    "            os.path.join(path, train), exts, fields, **kwargs)\n",
    "        val_data = None if validation is None else cls(\n",
    "            os.path.join(path, validation), exts, fields, **kwargs)\n",
    "        test_data = None if test is None else cls(\n",
    "            os.path.join(path, test), exts, fields, **kwargs)\n",
    "        return tuple(d for d in (train_data, val_data, test_data)\n",
    "                     if d is not None)\n",
    "    def read_conllu(self,path):\n",
    "        \"\"\"\n",
    "        creates three lists: one with the sentences, and two that represent the edges fot he graph\n",
    "        Argmunets:\n",
    "            path: path to a file with sentences in the ConLL-U standard\n",
    "        \"\"\"\n",
    "        f=open(path,'r')\n",
    "        lines=f.readlines()\n",
    "        f.close()\n",
    "        words=[]\n",
    "        origins=[]\n",
    "        targets=[]\n",
    "        edges=[]\n",
    "        temp_words=[]\n",
    "        temp_origins=[]\n",
    "        temp_targets=[]\n",
    "        temp_edges=[]\n",
    "        for line in lines:\n",
    "            if line=='\\n'or line=='':\n",
    "                words.append(temp_words)\n",
    "                origins.append(np.array(temp_origins))\n",
    "                targets.append(np.array(temp_targets))\n",
    "                edges.append(temp_edges)\n",
    "        \n",
    "                temp_words=[]\n",
    "                temp_origins=[]\n",
    "                temp_targets=[]\n",
    "                temp_edges=[]\n",
    "    \n",
    "            else:\n",
    "                splits=line.split('\\t')\n",
    "                temp_words.append(splits[1])\n",
    "                temp_origins.append(int(splits[0]))\n",
    "                temp_targets.append(int(splits[6]))\n",
    "                temp_edges.append(\"<\"+splits[7]+\">\")\n",
    "        for i in range(len(words)):\n",
    "            new_origins=origins[i]-1\n",
    "            edges_positions=np.arange(len(words[i]),2*len(words[i]))\n",
    "            new_targets=edges_positions.copy()\n",
    "            \n",
    "            edge_targets=targets[i]-1\n",
    "            root_pos=np.argmin(edge_targets)\n",
    "            edge_targets = np.delete(edge_targets, [root_pos])\n",
    "            edge_origins = np.delete(edges_positions,[root_pos])\n",
    "            origins[i] = [str(num) for num in list(np.concatenate((new_origins,edge_origins)))]\n",
    "            targets[i] = [str(num) for num in list(np.concatenate((new_targets,edge_targets)))]\n",
    "            assert len(targets[i])==len(origins[i])\n",
    "            words[i]=words[i]+edges[i]\n",
    "            \n",
    "        return words,origins,targets\n",
    "    \n",
    "    def read_text_file(self,path):\n",
    "        \"\"\"Read a text file \n",
    "        Argmunets:\n",
    "            path: path to a normal txt file\n",
    "        \"\"\"\n",
    "        f=open(path,'r')\n",
    "        lines=f.readlines()\n",
    "        f.close()\n",
    "        return lines\n",
    "    def gen_pe(self,words,org,trg,root_kw=\"<root>\"):\n",
    "        \"\"\"Calculates the min distance to the root to each node using BFS\n",
    "        Argmunets:\n",
    "            words: all the words of the sentence\n",
    "            org: a list with the origin of each edge\n",
    "            trg: a list with the target of each edge\n",
    "            root_kw: the keyword of the roo tag in the sentence\n",
    "        \"\"\"\n",
    "        start=None\n",
    "        for ind,word in enumerate(words):\n",
    "            if word==root_kw:\n",
    "                start=ind\n",
    "                continue\n",
    "        assert start!=None,\"sentence does not have a <root> tag\"\n",
    "        visited=[start]\n",
    "        distance_queue=[1]\n",
    "        distances=['0']*len(words)\n",
    "        while len(visited)!=0:\n",
    "            for index,node in enumerate(trg):\n",
    "                if str(node)==str(visited[0]):\n",
    "                    distances[int(org[index])]=str(distance_queue[0])\n",
    "                    visited.append(org[index])\n",
    "                    distance_queue.append(distance_queue[0]+1)\n",
    "            visited.pop(0)\n",
    "            distance_queue.pop(0)\n",
    "        return distances\n",
    "    def gen_pes(self,source_words,orgs,trgs,root_kw=\"<root>\"):\n",
    "        \"\"\"\n",
    "        Generates the positional embeddings for all the sentences in the dataset\n",
    "        argmunets:\n",
    "            source_words: a list of sentences \n",
    "            orgs: a list of lists of ede origins\n",
    "            trgs: a list of lists of the edge targets\n",
    "            root_kw:the keyword of the root tag in the senteces\n",
    "        \"\"\"\n",
    "        pes=[]\n",
    "        for i in range(len(source_words)):\n",
    "            pes.append(self.gen_pe(source_words[i],orgs[i],trgs[i],root_kw))\n",
    "        return pes\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_UNK_ID = lambda: 0\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "BOS_TOKEN = \"<BOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "class Vocabulary:\n",
    "    \"\"\" Vocabulary represents mapping between tokens and indices. \"\"\"\n",
    "\n",
    "    def __init__(self, tokens = None, file: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Create vocabulary from list of tokens or file.\n",
    "\n",
    "        Special tokens are added if not already in file or list.\n",
    "        File format: token with index i is in line i.\n",
    "\n",
    "        :param tokens: list of tokens\n",
    "        :param file: file to load vocabulary from\n",
    "        \"\"\"\n",
    "        # don't rename stoi and itos since needed for torchtext\n",
    "        # warning: stoi grows with unknown tokens, don't use for saving or size\n",
    "\n",
    "        # special symbols\n",
    "        self.specials = [\"<UNK>\", \"<PAD>\", \"<BOS>\", \"<EOS>\"]\n",
    "\n",
    "        self.stoi = defaultdict(DEFAULT_UNK_ID)\n",
    "        self.itos = []\n",
    "        if tokens is not None:\n",
    "            self._from_list(tokens)\n",
    "        elif file is not None:\n",
    "            self._from_file(file)\n",
    "\n",
    "    def _from_list(self, tokens = None) -> None:\n",
    "        \"\"\"\n",
    "        Make vocabulary from list of tokens.\n",
    "        Tokens are assumed to be unique and pre-selected.\n",
    "        Special symbols are added if not in list.\n",
    "\n",
    "        :param tokens: list of tokens\n",
    "        \"\"\"\n",
    "        self.add_tokens(tokens=self.specials+tokens)\n",
    "        assert len(self.stoi) == len(self.itos)\n",
    "\n",
    "    def _from_file(self, file: str) -> None:\n",
    "        \"\"\"\n",
    "        Make vocabulary from contents of file.\n",
    "        File format: token with index i is in line i.\n",
    "\n",
    "        :param file: path to file where the vocabulary is loaded from\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        with open(file, \"r\") as open_file:\n",
    "            for line in open_file:\n",
    "                tokens.append(line.strip(\"\\n\"))\n",
    "        self._from_list(tokens)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.stoi.__str__()\n",
    "\n",
    "    def to_file(self, file: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the vocabulary to a file, by writing token with index i in line i.\n",
    "\n",
    "        :param file: path to file where the vocabulary is written\n",
    "        \"\"\"\n",
    "        with open(file, \"w\") as open_file:\n",
    "            for t in self.itos:\n",
    "                open_file.write(\"{}\\n\".format(t))\n",
    "\n",
    "    def add_tokens(self, tokens) -> None:\n",
    "        \"\"\"\n",
    "        Add list of tokens to vocabulary\n",
    "\n",
    "        :param tokens: list of tokens to add to the vocabulary\n",
    "        \"\"\"\n",
    "        for t in tokens:\n",
    "            new_index = len(self.itos)\n",
    "            # add to vocab if not already there\n",
    "            if t not in self.itos:\n",
    "                self.itos.append(t)\n",
    "                self.stoi[t] = new_index\n",
    "\n",
    "    def is_unk(self, token: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether a token is covered by the vocabulary\n",
    "\n",
    "        :param token:\n",
    "        :return: True if covered, False otherwise\n",
    "        \"\"\"\n",
    "        return self.stoi[token] == DEFAULT_UNK_ID()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.itos)\n",
    "\n",
    "    def array_to_sentence(self, array: np.array, cut_at_eos=True,\n",
    "                          skip_pad=True):\n",
    "        \"\"\"\n",
    "        Converts an array of IDs to a sentence, optionally cutting the result\n",
    "        off at the end-of-sequence token.\n",
    "\n",
    "        :param array: 1D array containing indices\n",
    "        :param cut_at_eos: cut the decoded sentences at the first <eos>\n",
    "        :param skip_pad: skip generated <pad> tokens\n",
    "        :return: list of strings (tokens)\n",
    "        \"\"\"\n",
    "        sentence = []\n",
    "        for i in array:\n",
    "            s = self.itos[i]\n",
    "            if cut_at_eos and s == EOS_TOKEN:\n",
    "                break\n",
    "            if skip_pad and s == PAD_TOKEN:\n",
    "                continue\n",
    "            sentence.append(s)\n",
    "        return sentence\n",
    "\n",
    "    def arrays_to_sentences(self, arrays: np.array, cut_at_eos=True,\n",
    "                            skip_pad=True):\n",
    "        \"\"\"\n",
    "        Convert multiple arrays containing sequences of token IDs to their\n",
    "        sentences, optionally cutting them off at the end-of-sequence token.\n",
    "\n",
    "        :param arrays: 2D array containing indices\n",
    "        :param cut_at_eos: cut the decoded sentences at the first <eos>\n",
    "        :param skip_pad: skip generated <pad> tokens\n",
    "        :return: list of list of strings (tokens)\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        for array in arrays:\n",
    "            sentences.append(\n",
    "                self.array_to_sentence(array=array, cut_at_eos=cut_at_eos,\n",
    "                                       skip_pad=skip_pad))\n",
    "        return sentences\n",
    "\n",
    "\n",
    "def build_vocab(field: str, max_size: int, min_freq: int, dataset: Dataset,\n",
    "                vocab_file: str = None) -> Vocabulary:\n",
    "    \"\"\"\n",
    "    Builds vocabulary for a torchtext `field` from given`dataset` or\n",
    "    `vocab_file`.\n",
    "\n",
    "    :param field: attribute e.g. \"src\"\n",
    "    :param max_size: maximum size of vocabulary\n",
    "    :param min_freq: minimum frequency for an item to be included\n",
    "    :param dataset: dataset to load data for field from\n",
    "    :param vocab_file: file to store the vocabulary,\n",
    "        if not None, load vocabulary from here\n",
    "    :return: Vocabulary created from either `dataset` or `vocab_file`\n",
    "    \"\"\"\n",
    "\n",
    "    if vocab_file is not None:\n",
    "        # load it from file\n",
    "        vocab = Vocabulary(file=vocab_file)\n",
    "    else:\n",
    "        # create newly\n",
    "        def filter_min(counter: Counter, min_freq: int):\n",
    "            \"\"\" Filter counter by min frequency \"\"\"\n",
    "            filtered_counter = Counter({t: c for t, c in counter.items()\n",
    "                                        if c >= min_freq})\n",
    "            return filtered_counter\n",
    "\n",
    "        def sort_and_cut(counter: Counter, limit: int):\n",
    "            \"\"\" Cut counter to most frequent,\n",
    "            sorted numerically and alphabetically\"\"\"\n",
    "            # sort by frequency, then alphabetically\n",
    "            tokens_and_frequencies = sorted(counter.items(),\n",
    "                                            key=lambda tup: tup[0])\n",
    "            tokens_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "            vocab_tokens = [i[0] for i in tokens_and_frequencies[:limit]]\n",
    "            return vocab_tokens\n",
    "\n",
    "        tokens = []\n",
    "        for i in dataset.examples:\n",
    "            if field == \"src\":\n",
    "                tokens.extend(i.src)\n",
    "            elif field == \"trg\":\n",
    "                tokens.extend(i.trg)\n",
    "                \n",
    "            ### add edge_tokens\n",
    "            elif field == \"edge_org\":\n",
    "                tokens.extend(i.edge_org)\n",
    "            elif field == \"edge_trg\":\n",
    "                tokens.extend(i.edge_trg)\n",
    "            elif field ==\"positional_en\":\n",
    "                tokens.extend(i.positional_en)\n",
    "                \n",
    "\n",
    "        counter = Counter(tokens)\n",
    "        if min_freq > -1:\n",
    "            counter = filter_min(counter, min_freq)\n",
    "        vocab_tokens = sort_and_cut(counter, max_size)\n",
    "        assert len(vocab_tokens) <= max_size\n",
    "\n",
    "        vocab = Vocabulary(tokens=vocab_tokens)\n",
    "        assert len(vocab) <= max_size + len(vocab.specials)\n",
    "        assert vocab.itos[DEFAULT_UNK_ID()] == UNK_TOKEN\n",
    "\n",
    "    # check for all except for UNK token whether they are OOVs\n",
    "    for s in vocab.specials[1:]:\n",
    "        assert not vocab.is_unk(s)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['3', '3', '1', '5', '5', '3', '3', '2', '2', '0', '4', '4', '2', '2'], ['1', '3', '3', '3', '0', '2', '2', '2'], ['3', '3', '3', '3', '3', '3', '1', '3', '2', '2', '2', '2', '2', '2', '0', '2'], ['3', '3', '1', '5', '3', '7', '7', '5', '5', '5', '5', '7', '11', '11', '11', '9', '3', '2', '2', '0', '4', '2', '6', '6', '4', '4', '4', '4', '6', '10', '10', '10', '8', '2'], ['3', '1', '5', '3', '5', '5', '5', '3', '5', '7', '5', '9', '9', '9', '7', '7', '3', '5', '3', '5', '7', '7', '7', '7', '5', '9', '7', '13', '11', '9', '9', '9', '7', '11', '9', '11', '13', '13', '11', '15', '15', '13', '3', '2', '0', '4', '2', '4', '4', '4', '2', '4', '6', '4', '8', '8', '8', '6', '6', '2', '4', '2', '4', '6', '6', '6', '6', '4', '8', '6', '12', '10', '8', '8', '8', '6', '10', '8', '10', '12', '12', '10', '14', '14', '12', '2'], ['3', '3', '3', '3', '3', '1', '3', '2', '2', '2', '2', '2', '0', '2'], ['3', '3', '3', '1', '5', '5', '5', '3', '7', '5', '7', '11', '11', '9', '3', '5', '5', '3', '3', '2', '2', '2', '0', '4', '4', '4', '2', '6', '4', '6', '10', '10', '8', '2', '4', '4', '2', '2'], ['3', '1', '9', '7', '5', '9', '7', '7', '9', '9', '7', '9', '5', '3', '7', '7', '7', '5', '9', '7', '7', '9', '9', '7', '11', '9', '3', '2', '0', '8', '6', '4', '8', '6', '6', '8', '8', '6', '8', '4', '2', '6', '6', '6', '4', '8', '6', '6', '8', '8', '6', '10', '8', '2'], ['3', '5', '5', '3', '5', '3', '3', '3', '1', '3', '5', '5', '3', '9', '7', '5', '9', '9', '7', '3', '2', '4', '4', '2', '4', '2', '2', '2', '0', '2', '4', '4', '2', '8', '6', '4', '8', '8', '6', '2'], ['3', '1', '5', '3', '7', '9', '7', '7', '9', '7', '5', '9', '9', '7', '3', '2', '0', '4', '2', '6', '8', '6', '6', '8', '6', '4', '8', '8', '6', '2'], ['3', '3', '1', '3', '5', '7', '7', '5', '7', '7', '5', '9', '7', '9', '11', '13', '11', '13', '17', '15', '3', '2', '2', '0', '2', '4', '6', '6', '4', '6', '6', '4', '8', '6', '8', '10', '12', '10', '12', '16', '14', '2'], ['3', '3', '3', '1', '3', '7', '7', '7', '5', '7', '11', '11', '9', '3', '5', '7', '5', '5', '5', '3', '7', '5', '5', '7', '9', '9', '7', '5', '7', '9', '7', '9', '11', '9', '11', '13', '13', '11', '13', '15', '15', '15', '15', '13', '15', '3', '2', '2', '2', '0', '2', '6', '6', '6', '4', '6', '10', '10', '8', '2', '4', '6', '4', '4', '4', '2', '6', '4', '4', '6', '8', '8', '6', '4', '6', '8', '6', '8', '10', '8', '10', '12', '12', '10', '12', '14', '14', '14', '14', '12', '14', '2'], ['3', '5', '3', '3', '1', '3', '3', '3', '2', '4', '2', '2', '0', '2', '2', '2'], ['3', '1', '5', '3', '3', '2', '0', '4', '2', '2'], ['3', '7', '5', '1', '7', '5', '5', '3', '7', '5', '7', '7', '5', '9', '7', '3', '2', '6', '4', '0', '6', '4', '4', '2', '6', '4', '6', '6', '4', '8', '6', '2'], ['3', '3', '3', '1', '5', '5', '5', '3', '3', '5', '5', '5', '5', '7', '5', '5', '3', '5', '9', '9', '9', '9', '9', '9', '7', '13', '11', '11', '11', '9', '3', '2', '2', '2', '0', '4', '4', '4', '2', '2', '4', '4', '4', '4', '6', '4', '4', '2', '4', '8', '8', '8', '8', '8', '8', '6', '12', '10', '10', '10', '8', '2'], ['5', '3', '1', '3', '5', '7', '7', '5', '9', '7', '5', '5', '3', '5', '7', '7', '5', '3', '4', '2', '0', '2', '4', '6', '6', '4', '8', '6', '4', '4', '2', '4', '6', '6', '4', '2']]\n",
      "[['7', '8', '9', '10', '11', '12', '13', '2', '2', '5', '5', '2', '2'], ['4', '5', '6', '7', '0', '0', '0'], ['8', '9', '10', '11', '12', '13', '14', '15', '6', '6', '6', '6', '6', '6', '6'], ['17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '2', '2', '4', '2', '7', '7', '4', '4', '4', '4', '10', '15', '15', '15', '11', '2'], ['43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '1', '3', '1', '7', '7', '7', '1', '7', '10', '7', '14', '14', '14', '10', '10', '1', '18', '1', '18', '19', '24', '24', '24', '18', '26', '24', '28', '29', '26', '32', '32', '24', '34', '32', '34', '38', '38', '34', '41', '41', '38', '1'], ['7', '8', '9', '10', '11', '12', '13', '5', '5', '5', '5', '5', '5'], ['19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '3', '3', '3', '7', '7', '7', '3', '9', '7', '9', '13', '13', '10', '3', '17', '17', '3', '3'], ['27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '1', '10', '4', '13', '6', '4', '4', '10', '10', '4', '10', '13', '1', '17', '17', '17', '13', '19', '17', '17', '23', '23', '17', '25', '23', '1'], ['20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '8', '3', '3', '8', '3', '8', '8', '8', '8', '12', '12', '8', '14', '15', '12', '18', '18', '15', '8'], ['15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '1', '3', '1', '10', '4', '10', '10', '9', '10', '3', '13', '13', '10', '1'], ['21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '2', '2', '2', '3', '7', '7', '3', '10', '10', '3', '12', '10', '12', '13', '16', '13', '16', '19', '17', '2'], ['46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '3', '3', '3', '3', '8', '8', '8', '4', '8', '12', '12', '9', '3', '19', '16', '19', '19', '19', '3', '21', '19', '19', '27', '23', '26', '27', '19', '27', '30', '27', '30', '33', '30', '33', '37', '37', '33', '37', '43', '43', '43', '43', '37', '43', '3'], ['8', '9', '10', '11', '12', '13', '14', '15', '4', '2', '4', '4', '4', '4', '4'], ['5', '6', '7', '8', '9', '1', '3', '1', '1'], ['16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '3', '2', '0', '5', '7', '7', '3', '9', '7', '12', '12', '7', '14', '12', '3'], ['31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '3', '3', '3', '7', '7', '7', '3', '3', '16', '16', '16', '16', '14', '16', '16', '3', '16', '24', '24', '24', '24', '24', '24', '17', '26', '29', '29', '29', '24', '3'], ['18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '1', '2', '2', '3', '7', '7', '3', '9', '7', '12', '12', '2', '12', '16', '16', '12', '2']]\n",
      "[['0', '1', '2', '3', '4', '5', '6', '7', '8', '10', '11', '12', '13'], ['0', '1', '2', '3', '5', '6', '7'], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '15'], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33'], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85'], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '13'], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37'], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53'], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39'], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29'], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41'], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91'], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '13', '14', '15'], ['0', '1', '2', '3', '4', '5', '7', '8', '9'], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31'], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61'], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35']]\n"
     ]
    }
   ],
   "source": [
    "english_sentences=[\"I am happy to be here\",\"This is the test sentence number 2\",\"the last test sentence is  this one\"]\n",
    "spanish_sentences=[\"estoy feliz de estar aquí\",\"ésta es la oración de prueba número 2\",\"La última ouración de prueba es ésta\"]\n",
    "edge_indexes=[[[0,1,2,3,4,5],[0,1,2,3,4,5]],[[0,1,2,3,4],[0,1,2,3,4]],[[0,1,2,3],[0,1,2,3]]]\n",
    "dataset=GraphTranslationDataset(\\\n",
    "                           \"/home/hec44/Documents/joeynmt/test/data/toy/dev.conll\",\\\n",
    "                           \"/home/hec44/Documents/joeynmt/test/data/toy/dev.de\",\\\n",
    "                           fields=(src_field, trg_field,edge_org_field,edge_trg_field,positional_en_field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        source_words,origins,targets=self.read_conllu(src_file)\n",
    "        pes=self.gen_pes(source_words,origins,targets)\n",
    "        \n",
    "        target_words=self.read_text_file(trg_file)\n",
    "\"\"\"\n",
    "\n",
    "src_vocab = build_vocab(field=\"src\", min_freq=0,\n",
    "                            max_size=99,\n",
    "                            dataset=dataset, vocab_file=None)\n",
    "trg_vocab = build_vocab(field=\"trg\", min_freq=0,\n",
    "                            max_size=99,\n",
    "                            dataset=dataset, vocab_file=None)\n",
    "edge_org_vocab = build_vocab(field=\"edge_org\", min_freq=0,\n",
    "                            max_size=99,\n",
    "                            dataset=dataset, vocab_file=None)\n",
    "edge_trg_vocab = build_vocab(field=\"edge_trg\", min_freq=0,\n",
    "                            max_size=99,\n",
    "                            dataset=dataset, vocab_file=None)\n",
    "positional_en_vocab = build_vocab(field=\"positional_en\", min_freq=0,\n",
    "                            max_size=99,\n",
    "                            dataset=dataset, vocab_file=None)\n",
    "src_field.vocab = src_vocab\n",
    "trg_field.vocab = trg_vocab\n",
    "edge_org_field.vocab = edge_org_vocab\n",
    "edge_trg_field.vocab = edge_trg_vocab\n",
    "positional_en_field.vocab = positional_en_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '3', '1', '5', '5', '3', '3', '2', '2', '0', '4', '4', '2', '2']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset.positional_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hec44/.local/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "data_iter = data.BucketIterator(\n",
    "            repeat=False, sort=False, dataset=dataset,\n",
    "            batch_size=3, batch_size_fn=None,\n",
    "            train=True, sort_within_batch=True,\n",
    "            sort_key=lambda x: len(x.src), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hec44/.local/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "batch=next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5,  5,  5,  5,  5, 15,  5,  4,  4,  4,  4,  4, 14,  4],\n",
       "         [ 5, 15,  7,  5,  5,  4, 14,  6,  4,  4,  0,  0,  0,  0],\n",
       "         [15,  5,  5,  5, 14,  4,  4,  4,  0,  0,  0,  0,  0,  0]]),\n",
       " tensor([14, 10,  8]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.positional_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(batch, \"positional_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fields = [('edge_org', data.Field(sequential=False)), ('edge_trg', data.Field(sequential=False))]\n",
    "data.Example.fromlist([[0,1,2,3,4,5],\\\n",
    "                       [1,2,3,4,5,0]],\\\n",
    "                      fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex=data.Field(sequential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('/home/hec44/Documents/joeynmt/test/data/toy/dev.conll','r')\n",
    "lines=f.readlines()\n",
    "f.close()\n",
    "words=[]\n",
    "origins=[]\n",
    "targets=[]\n",
    "edges=[]\n",
    "temp_words=[]\n",
    "temp_origins=[]\n",
    "temp_targets=[]\n",
    "temp_edges=[]\n",
    "for line in lines:\n",
    "    if line=='\\n'or line=='':\n",
    "        words.append(temp_words)\n",
    "        origins.append(np.array(temp_origins))\n",
    "        targets.append(np.array(temp_targets))\n",
    "        edges.append(temp_edges)\n",
    "        \n",
    "        temp_words=[]\n",
    "        temp_origins=[]\n",
    "        temp_targets=[]\n",
    "        temp_edges=[]\n",
    "    \n",
    "    else:\n",
    "        splits=line.split('\\t')\n",
    "        temp_words.append(splits[1])\n",
    "        temp_origins.append(int(splits[0]))\n",
    "        temp_targets.append(int(splits[6]))\n",
    "        temp_edges.append(\"<\"+splits[7]+\">\")\n",
    "        \n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words)):\n",
    "    new_origins=origins[i]-1\n",
    "    edges_positions=np.arange(len(words[i]),2*len(words[i]))\n",
    "    new_targets=edges_positions.copy()\n",
    "    \n",
    "    edge_targets=targets[i]-1\n",
    "    root_pos=np.argmin(edge_targets)\n",
    "    edge_targets = np.delete(edge_targets, [root_pos])\n",
    "    edge_origins = np.delete(edges_positions,[root_pos])\n",
    "    origins[i]=list(np.concatenate((new_origins,edge_origins)))\n",
    "    targets[i]=list(np.concatenate((new_targets,edge_targets)))\n",
    "    words[i]=words[i]+edges[i]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('/home/hec44/Documents/joeynmt/test/data/toy/dev.conll','r')\n",
    "lines=f.readlines()\n",
    "f.close()\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pe(words,org,trg,root_kw=\"<root>\"):\n",
    "    start=None\n",
    "    for ind,word in enumerate(words):\n",
    "        if word==root_kw:\n",
    "            start=ind\n",
    "            continue\n",
    "    assert start!=None,\"sentence does not have a <root> tag\"\n",
    "    visited=[start]\n",
    "    distance_queue=[1]\n",
    "    distances=[0]*len(words)\n",
    "    while len(visited)!=0:\n",
    "        for index,node in enumerate(trg):\n",
    "            if node==visited[0]:\n",
    "                distances[org[index]]=distance_queue[0]\n",
    "                visited.append(org[index])\n",
    "                distance_queue.append(distance_queue[0]+1)\n",
    "        visited.pop(0)\n",
    "        distance_queue.pop(0)\n",
    "    return distances\n",
    "gen_pe(words[0],origins[0],targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test={}\n",
    "test['a']={'b':0}\n",
    "test['c']={'d':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['a']['b']['d'].get('dfcdf',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
