{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext.data import Dataset, Iterator, Field\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hec44/.local/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "tok_fun = lambda s: s.split()\n",
    "\n",
    "src_field = data.Field(init_token=None, eos_token=\"<EOS>\",\n",
    "                           pad_token=\"<PAD>\", tokenize=tok_fun,\n",
    "                           batch_first=True, lower=True,\n",
    "                           unk_token=\"<UNK>\",\n",
    "                           include_lengths=True)\n",
    "\n",
    "trg_field = data.Field(init_token=\"<BOS>\", eos_token=\"<EOS>\",\n",
    "                           pad_token=\"<PAD>\", tokenize=tok_fun,\n",
    "                           unk_token=\"<UNK>\",\n",
    "                           batch_first=True, lower=True,\n",
    "                           include_lengths=True)\n",
    "edge_org_field = data.Field(init_token=None, eos_token=None,\n",
    "                           pad_token=None ,tokenize=tok_fun,\n",
    "                           unk_token=None,\n",
    "                           batch_first=True, lower=True,\n",
    "                           include_lengths=True)\n",
    "edge_trg_field = data.Field(init_token=None, eos_token=None,\n",
    "                           pad_token=None, tokenize=tok_fun,\n",
    "                           unk_token=None,\n",
    "                           batch_first=True, lower=True,\n",
    "                           include_lengths=True)\n",
    "positional_en_field = data.Field(init_token=None, eos_token=None,\n",
    "                           pad_token=None, tokenize=tok_fun,\n",
    "                           unk_token=None,\n",
    "                           batch_first=True, lower=True,\n",
    "                           include_lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.field.Field object at 0x7f74691b4ee0>\n"
     ]
    }
   ],
   "source": [
    "print(src_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(data.Dataset):\n",
    "    \"\"\"Defines a dataset for machine translation.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return data.interleave_keys(len(ex.src), len(ex.trg))\n",
    "\n",
    "    def __init__(self, src_file, trg_file, fields, **kwargs):\n",
    "        \"\"\"Create a TranslationDataset given paths and fields.\n",
    "        Arguments:\n",
    "            path: Common prefix of paths to the data files for both languages.\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            Remaining keyword arguments: Passed to the constructor of\n",
    "                data.Dataset.\n",
    "        \"\"\"\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            fields = [('src', fields[0]), ('trg', fields[1]),\\\n",
    "                     ('edge_org', fields[2]), ('edge_trg', fields[3]), ('positional_en',fields[4])]\n",
    "\n",
    "        examples = []\n",
    "        source_words,origins,targets=self.read_conllu(src_file)\n",
    "        pes=self.gen_pes(source_words,origins,targets)\n",
    "        \n",
    "        target_words=self.read_text_file(trg_file)\n",
    "        #pdb.set_trace()\n",
    "        #assert len(source_words)==len(target_words),\"Mismatch of source and tagret sentences\"\n",
    "\n",
    "        for i in range(len(source_words)):\n",
    "                src_line, trg_line = \" \".join(source_words[i]),target_words[i]\n",
    "                src_line, trg_line = src_line.strip(), trg_line.strip()\n",
    "                if src_line != '' and trg_line != '':\n",
    "                    examples.append(data.Example.fromlist(\n",
    "                        [src_line, trg_line,\" \".join(origins[i]),\" \".join(target_words[i]),\\\n",
    "                        \" \".join(pes[i])],\\\n",
    "                        fields))\n",
    "        super(TranslationDataset, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, exts, fields, path=None, root='.data',\n",
    "               train='train', validation='val', test='test', **kwargs):\n",
    "        \"\"\"Create dataset objects for splits of a TranslationDataset.\n",
    "        Arguments:\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            path (str): Common prefix of the splits' file paths, or None to use\n",
    "                the result of cls.download(root).\n",
    "            root: Root dataset storage directory. Default is '.data'.\n",
    "            train: The prefix of the train data. Default: 'train'.\n",
    "            validation: The prefix of the validation data. Default: 'val'.\n",
    "            test: The prefix of the test data. Default: 'test'.\n",
    "            Remaining keyword arguments: Passed to the splits method of\n",
    "                Dataset.\n",
    "        \"\"\"\n",
    "        if path is None:\n",
    "            path = cls.download(root)\n",
    "\n",
    "        train_data = None if train is None else cls(\n",
    "            os.path.join(path, train), exts, fields, **kwargs)\n",
    "        val_data = None if validation is None else cls(\n",
    "            os.path.join(path, validation), exts, fields, **kwargs)\n",
    "        test_data = None if test is None else cls(\n",
    "            os.path.join(path, test), exts, fields, **kwargs)\n",
    "        return tuple(d for d in (train_data, val_data, test_data)\n",
    "                     if d is not None)\n",
    "    def read_conllu(self,path):\n",
    "        f=open(path,'r')\n",
    "        lines=f.readlines()\n",
    "        f.close()\n",
    "        words=[]\n",
    "        origins=[]\n",
    "        targets=[]\n",
    "        edges=[]\n",
    "        temp_words=[]\n",
    "        temp_origins=[]\n",
    "        temp_targets=[]\n",
    "        temp_edges=[]\n",
    "        for line in lines:\n",
    "            if line=='\\n'or line=='':\n",
    "                words.append(temp_words)\n",
    "                origins.append(np.array(temp_origins))\n",
    "                targets.append(np.array(temp_targets))\n",
    "                edges.append(temp_edges)\n",
    "        \n",
    "                temp_words=[]\n",
    "                temp_origins=[]\n",
    "                temp_targets=[]\n",
    "                temp_edges=[]\n",
    "    \n",
    "            else:\n",
    "                splits=line.split('\\t')\n",
    "                temp_words.append(splits[1])\n",
    "                temp_origins.append(int(splits[0]))\n",
    "                temp_targets.append(int(splits[6]))\n",
    "                temp_edges.append(\"<\"+splits[7]+\">\")\n",
    "        for i in range(len(words)):\n",
    "            new_origins=origins[i]-1\n",
    "            edges_positions=np.arange(len(words[i]),2*len(words[i]))\n",
    "            new_targets=edges_positions.copy()\n",
    "            \n",
    "            edge_targets=targets[i]-1\n",
    "            root_pos=np.argmin(edge_targets)\n",
    "            edge_targets = np.delete(edge_targets, [root_pos])\n",
    "            edge_origins = np.delete(edges_positions,[root_pos])\n",
    "            origins[i] = [str(num) for num in list(np.concatenate((new_origins,edge_origins)))]\n",
    "            targets[i] = [str(num) for num in list(np.concatenate((new_targets,edge_targets)))]\n",
    "            words[i]=words[i]+edges[i]\n",
    "            \n",
    "        return words,origins,targets\n",
    "    \n",
    "    def read_text_file(self,path):\n",
    "        f=open(path,'r')\n",
    "        lines=f.readlines()\n",
    "        f.close()\n",
    "        return lines\n",
    "    def gen_pe(self,words,org,trg,root_kw=\"<root>\"):\n",
    "        start=None\n",
    "        for ind,word in enumerate(words):\n",
    "            if word==root_kw:\n",
    "                start=ind\n",
    "                continue\n",
    "        assert start!=None,\"sentence does not have a <root> tag\"\n",
    "        visited=[start]\n",
    "        distance_queue=[1]\n",
    "        distances=[0]*len(words)\n",
    "        while len(visited)!=0:\n",
    "            for index,node in enumerate(trg):\n",
    "                if node==visited[0]:\n",
    "                    distances[org[index]]=str(distance_queue[0])\n",
    "                    visited.append(org[index])\n",
    "                    distance_queue.append(distance_queue[0]+1)\n",
    "            visited.pop(0)\n",
    "            distance_queue.pop(0)\n",
    "        return distances\n",
    "    def gen_pes(self,source_words,orgs,trgs,root_kw=\"<root>\"):\n",
    "        pes=[]\n",
    "        for i in range(len(source_words)):\n",
    "            pes.append(self.gen_pe(source_words[i],orgs[i],trgs[i],root_kw))\n",
    "        return pes\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_UNK_ID = lambda: 0\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "BOS_TOKEN = \"<BOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "class Vocabulary:\n",
    "    \"\"\" Vocabulary represents mapping between tokens and indices. \"\"\"\n",
    "\n",
    "    def __init__(self, tokens = None, file: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Create vocabulary from list of tokens or file.\n",
    "\n",
    "        Special tokens are added if not already in file or list.\n",
    "        File format: token with index i is in line i.\n",
    "\n",
    "        :param tokens: list of tokens\n",
    "        :param file: file to load vocabulary from\n",
    "        \"\"\"\n",
    "        # don't rename stoi and itos since needed for torchtext\n",
    "        # warning: stoi grows with unknown tokens, don't use for saving or size\n",
    "\n",
    "        # special symbols\n",
    "        self.specials = [\"<UNK>\", \"<PAD>\", \"<BOS>\", \"<EOS>\"]\n",
    "\n",
    "        self.stoi = defaultdict(DEFAULT_UNK_ID)\n",
    "        self.itos = []\n",
    "        if tokens is not None:\n",
    "            self._from_list(tokens)\n",
    "        elif file is not None:\n",
    "            self._from_file(file)\n",
    "\n",
    "    def _from_list(self, tokens = None) -> None:\n",
    "        \"\"\"\n",
    "        Make vocabulary from list of tokens.\n",
    "        Tokens are assumed to be unique and pre-selected.\n",
    "        Special symbols are added if not in list.\n",
    "\n",
    "        :param tokens: list of tokens\n",
    "        \"\"\"\n",
    "        self.add_tokens(tokens=self.specials+tokens)\n",
    "        assert len(self.stoi) == len(self.itos)\n",
    "\n",
    "    def _from_file(self, file: str) -> None:\n",
    "        \"\"\"\n",
    "        Make vocabulary from contents of file.\n",
    "        File format: token with index i is in line i.\n",
    "\n",
    "        :param file: path to file where the vocabulary is loaded from\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        with open(file, \"r\") as open_file:\n",
    "            for line in open_file:\n",
    "                tokens.append(line.strip(\"\\n\"))\n",
    "        self._from_list(tokens)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.stoi.__str__()\n",
    "\n",
    "    def to_file(self, file: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the vocabulary to a file, by writing token with index i in line i.\n",
    "\n",
    "        :param file: path to file where the vocabulary is written\n",
    "        \"\"\"\n",
    "        with open(file, \"w\") as open_file:\n",
    "            for t in self.itos:\n",
    "                open_file.write(\"{}\\n\".format(t))\n",
    "\n",
    "    def add_tokens(self, tokens) -> None:\n",
    "        \"\"\"\n",
    "        Add list of tokens to vocabulary\n",
    "\n",
    "        :param tokens: list of tokens to add to the vocabulary\n",
    "        \"\"\"\n",
    "        for t in tokens:\n",
    "            new_index = len(self.itos)\n",
    "            # add to vocab if not already there\n",
    "            if t not in self.itos:\n",
    "                self.itos.append(t)\n",
    "                self.stoi[t] = new_index\n",
    "\n",
    "    def is_unk(self, token: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether a token is covered by the vocabulary\n",
    "\n",
    "        :param token:\n",
    "        :return: True if covered, False otherwise\n",
    "        \"\"\"\n",
    "        return self.stoi[token] == DEFAULT_UNK_ID()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.itos)\n",
    "\n",
    "    def array_to_sentence(self, array: np.array, cut_at_eos=True,\n",
    "                          skip_pad=True):\n",
    "        \"\"\"\n",
    "        Converts an array of IDs to a sentence, optionally cutting the result\n",
    "        off at the end-of-sequence token.\n",
    "\n",
    "        :param array: 1D array containing indices\n",
    "        :param cut_at_eos: cut the decoded sentences at the first <eos>\n",
    "        :param skip_pad: skip generated <pad> tokens\n",
    "        :return: list of strings (tokens)\n",
    "        \"\"\"\n",
    "        sentence = []\n",
    "        for i in array:\n",
    "            s = self.itos[i]\n",
    "            if cut_at_eos and s == EOS_TOKEN:\n",
    "                break\n",
    "            if skip_pad and s == PAD_TOKEN:\n",
    "                continue\n",
    "            sentence.append(s)\n",
    "        return sentence\n",
    "\n",
    "    def arrays_to_sentences(self, arrays: np.array, cut_at_eos=True,\n",
    "                            skip_pad=True):\n",
    "        \"\"\"\n",
    "        Convert multiple arrays containing sequences of token IDs to their\n",
    "        sentences, optionally cutting them off at the end-of-sequence token.\n",
    "\n",
    "        :param arrays: 2D array containing indices\n",
    "        :param cut_at_eos: cut the decoded sentences at the first <eos>\n",
    "        :param skip_pad: skip generated <pad> tokens\n",
    "        :return: list of list of strings (tokens)\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        for array in arrays:\n",
    "            sentences.append(\n",
    "                self.array_to_sentence(array=array, cut_at_eos=cut_at_eos,\n",
    "                                       skip_pad=skip_pad))\n",
    "        return sentences\n",
    "\n",
    "\n",
    "def build_vocab(field: str, max_size: int, min_freq: int, dataset: Dataset,\n",
    "                vocab_file: str = None) -> Vocabulary:\n",
    "    \"\"\"\n",
    "    Builds vocabulary for a torchtext `field` from given`dataset` or\n",
    "    `vocab_file`.\n",
    "\n",
    "    :param field: attribute e.g. \"src\"\n",
    "    :param max_size: maximum size of vocabulary\n",
    "    :param min_freq: minimum frequency for an item to be included\n",
    "    :param dataset: dataset to load data for field from\n",
    "    :param vocab_file: file to store the vocabulary,\n",
    "        if not None, load vocabulary from here\n",
    "    :return: Vocabulary created from either `dataset` or `vocab_file`\n",
    "    \"\"\"\n",
    "\n",
    "    if vocab_file is not None:\n",
    "        # load it from file\n",
    "        vocab = Vocabulary(file=vocab_file)\n",
    "    else:\n",
    "        # create newly\n",
    "        def filter_min(counter: Counter, min_freq: int):\n",
    "            \"\"\" Filter counter by min frequency \"\"\"\n",
    "            filtered_counter = Counter({t: c for t, c in counter.items()\n",
    "                                        if c >= min_freq})\n",
    "            return filtered_counter\n",
    "\n",
    "        def sort_and_cut(counter: Counter, limit: int):\n",
    "            \"\"\" Cut counter to most frequent,\n",
    "            sorted numerically and alphabetically\"\"\"\n",
    "            # sort by frequency, then alphabetically\n",
    "            tokens_and_frequencies = sorted(counter.items(),\n",
    "                                            key=lambda tup: tup[0])\n",
    "            tokens_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "            vocab_tokens = [i[0] for i in tokens_and_frequencies[:limit]]\n",
    "            return vocab_tokens\n",
    "\n",
    "        tokens = []\n",
    "        for i in dataset.examples:\n",
    "            if field == \"src\":\n",
    "                tokens.extend(i.src)\n",
    "            elif field == \"trg\":\n",
    "                tokens.extend(i.trg)\n",
    "                \n",
    "            ### add edge_tokens\n",
    "            elif field == \"edge_org\":\n",
    "                tokens.extend(i.edge_org)\n",
    "            elif field == \"edge_trg\":\n",
    "                tokens.extend(i.edge_trg)\n",
    "            elif field ==\"positional_en\":\n",
    "                tokens.extend(i.positional_en)\n",
    "                \n",
    "\n",
    "        counter = Counter(tokens)\n",
    "        if min_freq > -1:\n",
    "            counter = filter_min(counter, min_freq)\n",
    "        vocab_tokens = sort_and_cut(counter, max_size)\n",
    "        assert len(vocab_tokens) <= max_size\n",
    "\n",
    "        vocab = Vocabulary(tokens=vocab_tokens)\n",
    "        assert len(vocab) <= max_size + len(vocab.specials)\n",
    "        assert vocab.itos[DEFAULT_UNK_ID()] == UNK_TOKEN\n",
    "\n",
    "    # check for all except for UNK token whether they are OOVs\n",
    "    for s in vocab.specials[1:]:\n",
    "        assert not vocab.is_unk(s)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b8a969cc8f3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspanish_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"estoy feliz de estar aquí\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ésta es la oración de prueba número 2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"La última ouración de prueba es ésta\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0medge_indexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m dataset=TranslationDataset(\\\n\u001b[0m\u001b[1;32m      5\u001b[0m                            \u001b[0;34m\"/home/hec44/Documents/joeynmt/test/data/toy/dev.conll\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                            \u001b[0;34m\"/home/hec44/Documents/joeynmt/test/data/toy/dev.de\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-2df3efc63d45>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src_file, trg_file, fields, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     examples.append(data.Example.fromlist(\n\u001b[1;32m     35\u001b[0m                         [src_line, trg_line,\" \".join(origins[i]),\" \".join(target_words[i]),\\\n\u001b[0;32m---> 36\u001b[0;31m                         \" \".join(pes[i])],\\\n\u001b[0m\u001b[1;32m     37\u001b[0m                         fields))\n\u001b[1;32m     38\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTranslationDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, int found"
     ]
    }
   ],
   "source": [
    "english_sentences=[\"I am happy to be here\",\"This is the test sentence number 2\",\"the last test sentence is  this one\"]\n",
    "spanish_sentences=[\"estoy feliz de estar aquí\",\"ésta es la oración de prueba número 2\",\"La última ouración de prueba es ésta\"]\n",
    "edge_indexes=[[[0,1,2,3,4,5],[0,1,2,3,4,5]],[[0,1,2,3,4],[0,1,2,3,4]],[[0,1,2,3],[0,1,2,3]]]\n",
    "dataset=TranslationDataset(\\\n",
    "                           \"/home/hec44/Documents/joeynmt/test/data/toy/dev.conll\",\\\n",
    "                           \"/home/hec44/Documents/joeynmt/test/data/toy/dev.de\",\\\n",
    "                           fields=(src_field, trg_field,edge_org_field,edge_trg_field,positional_en_field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        source_words,origins,targets=self.read_conllu(src_file)\n",
    "        pes=self.gen_pes(source_words,origins,targets)\n",
    "        \n",
    "        target_words=self.read_text_file(trg_file)\n",
    "\n",
    "src_vocab = build_vocab(field=\"src\", min_freq=1,\n",
    "                            max_size=99,\n",
    "                            dataset=dataset, vocab_file=None)\n",
    "trg_vocab = build_vocab(field=\"trg\", min_freq=1,\n",
    "                            max_size=99,\n",
    "                            dataset=dataset, vocab_file=None)\n",
    "edge_org_vocab = build_vocab(field=\"edge_org\", min_freq=0,\n",
    "                            max_size=99,\n",
    "                            dataset=dataset, vocab_file=None)\n",
    "edge_trg_vocab = build_vocab(field=\"edge_trg\", min_freq=0,\n",
    "                            max_size=99,\n",
    "                            dataset=dataset, vocab_file=None)\n",
    "positional_en_vocab = build_vocab(field=\"positional_en\", min_freq=0,\n",
    "                            max_size=99,\n",
    "                            dataset=dataset, vocab_file=None)\n",
    "src_field.vocab = src_vocab\n",
    "trg_field.vocab = trg_vocab\n",
    "edge_org_field.vocab = edge_org_vocab\n",
    "edge_trg_field.vocab = edge_trg_vocab\n",
    "position_en.field.vocab = positional_en_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_trg_vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = data.BucketIterator(\n",
    "            repeat=False, sort=False, dataset=dataset,\n",
    "            batch_size=3, batch_size_fn=None,\n",
    "            train=True, sort_within_batch=True,\n",
    "            sort_key=lambda x: len(x.src), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.edge_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fields = [('edge_org', data.Field(sequential=False)), ('edge_trg', data.Field(sequential=False))]\n",
    "data.Example.fromlist([[0,1,2,3,4,5],\\\n",
    "                       [1,2,3,4,5,0]],\\\n",
    "                      fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex=data.Field(sequential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('/home/hec44/Documents/joeynmt/test/data/toy/dev.conll','r')\n",
    "lines=f.readlines()\n",
    "f.close()\n",
    "words=[]\n",
    "origins=[]\n",
    "targets=[]\n",
    "edges=[]\n",
    "temp_words=[]\n",
    "temp_origins=[]\n",
    "temp_targets=[]\n",
    "temp_edges=[]\n",
    "for line in lines:\n",
    "    if line=='\\n'or line=='':\n",
    "        words.append(temp_words)\n",
    "        origins.append(np.array(temp_origins))\n",
    "        targets.append(np.array(temp_targets))\n",
    "        edges.append(temp_edges)\n",
    "        \n",
    "        temp_words=[]\n",
    "        temp_origins=[]\n",
    "        temp_targets=[]\n",
    "        temp_edges=[]\n",
    "    \n",
    "    else:\n",
    "        splits=line.split('\\t')\n",
    "        temp_words.append(splits[1])\n",
    "        temp_origins.append(int(splits[0]))\n",
    "        temp_targets.append(int(splits[6]))\n",
    "        temp_edges.append(\"<\"+splits[7]+\">\")\n",
    "        \n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words)):\n",
    "    new_origins=origins[i]-1\n",
    "    edges_positions=np.arange(len(words[i]),2*len(words[i]))\n",
    "    new_targets=edges_positions.copy()\n",
    "    \n",
    "    edge_targets=targets[i]-1\n",
    "    root_pos=np.argmin(edge_targets)\n",
    "    edge_targets = np.delete(edge_targets, [root_pos])\n",
    "    edge_origins = np.delete(edges_positions,[root_pos])\n",
    "    origins[i]=list(np.concatenate((new_origins,edge_origins)))\n",
    "    targets[i]=list(np.concatenate((new_targets,edge_targets)))\n",
    "    words[i]=words[i]+edges[i]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('/home/hec44/Documents/joeynmt/test/data/toy/dev.conll','r')\n",
    "lines=f.readlines()\n",
    "f.close()\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pe(words,org,trg,root_kw=\"<root>\"):\n",
    "    start=None\n",
    "    for ind,word in enumerate(words):\n",
    "        if word==root_kw:\n",
    "            start=ind\n",
    "            continue\n",
    "    assert start!=None,\"sentence does not have a <root> tag\"\n",
    "    visited=[start]\n",
    "    distance_queue=[1]\n",
    "    distances=[0]*len(words)\n",
    "    while len(visited)!=0:\n",
    "        for index,node in enumerate(trg):\n",
    "            if node==visited[0]:\n",
    "                distances[org[index]]=distance_queue[0]\n",
    "                visited.append(org[index])\n",
    "                distance_queue.append(distance_queue[0]+1)\n",
    "        visited.pop(0)\n",
    "        distance_queue.pop(0)\n",
    "    return distances\n",
    "gen_pe(words[0],origins[0],targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
